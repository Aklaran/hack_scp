{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hack SCP Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing KL \n",
    "\n",
    "Through our webscraping efforts, we collected 10 CSV's of translated/original text pair data.  \n",
    "\n",
    "Entropy is computed using the SciPy library, as illustrated below: \n",
    "\n",
    "1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get the probability distributions of two different texts, and calculate using the scipy.entropy function: __S = sum(pk * log(pk / qk))__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated entropies are:  2.0 ,  1.3567796494470397\n"
     ]
    }
   ],
   "source": [
    "# Prb distributions\n",
    "prb1 = [.25, .25, .25, .25]\n",
    "prb2 = [.1, .1, .1, .7]\n",
    "\n",
    "entropy1 = entropy(prb1, base=2)\n",
    "entropy2 = entropy(prb2, base=2)\n",
    "\n",
    "print(f\"The calculated entropies are: \", entropy1, \", \", entropy2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Scraped Data:\n",
    "\n",
    "We used a basic python script to calculate given entropies, and do some minor data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell is a markdown so no current data gets overriden:\n",
    "\n",
    "\n",
    "import sys, pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "\n",
    "def getEntropy(file, splitOnChars = False):\n",
    "\ttry:\n",
    "\t\tfile1 = open(file, 'r',encoding=\"utf8\")\n",
    "\texcept:\n",
    "\t\treturn \"None\"\n",
    "\n",
    "\n",
    "\tif splitOnChars: \n",
    "\t\ttext1 = list(file1.read()) # Need to split on characters for languages like chinese\n",
    "\telse: \n",
    "\t\ttext1 = file1.read().split(\" \")\n",
    "\n",
    "\t# Get probability distributions\n",
    "\tdist1 = {}\n",
    "\tfor word in text1:\n",
    "\t\tdist1[word] = dist1.get(word, 0) + 1\n",
    "\n",
    "\t# Normalize\n",
    "\tfor word in dist1:\n",
    "\t\tdist1[word] = (dist1[word] / len(dist1))\n",
    "\n",
    "\ttext1_data = list(dist1.values())\n",
    "\n",
    "\treturn entropy(text1_data) \n",
    "\n",
    "def parse(file, parseChars = False):\n",
    "\tdf = pd.read_csv(file)\n",
    "\tlanguageMap = {\"cn\":\"chinese\",\n",
    "\t\t\t\t\t\"jp\":\"japanese\",\n",
    "\t\t\t\t\t\"fr\":\"french\",\n",
    "\t\t\t\t\t\"de\":\"german\",\n",
    "\t\t\t\t\t\"it\":\"italian\",\n",
    "\t\t\t\t\t\"ko\":\"korean\",\n",
    "\t\t\t\t\t\"pt\":\"portuguese\",\n",
    "\t\t\t\t\t\"ru\":\"russian\",\n",
    "\t\t\t\t\t\"es\":\"spanish\",\n",
    "\t\t\t\t\t\"th\":\"thai\"}\n",
    "\n",
    "\toriginalCol = []\n",
    "\ttranslateCol = []\n",
    "\tdifferenceCol = []\n",
    "\tfor language in languageMap:\n",
    "\t\tif language in df[\"href\"][1]:\n",
    "\t\t\tlang = languageMap[language]\n",
    "\t\t\tbreak\n",
    "\tfor i in df.index:\n",
    "\t\topath = \"data/\" + lang + \"Originals\" + df[\"href\"][i] + \".txt\"\n",
    "\t\ttpath = \"data/\" + \"englishFrom\" + lang + df[\"href\"][i] + \".txt\"\n",
    "\n",
    "\t\t# Cleaning any E notation syntax using the double() func \n",
    "\t\ttry:\n",
    "\t\t\torigString = double(getEntropy(opath, parseChars))\n",
    "\t\texcept Exception as e: \n",
    "\t\t\torigString = getEntropy(opath, parseChars)\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\ttransString = double(getEntropy(tpath, parseChars))\n",
    "\t\texcept Exception as e:\n",
    "\t\t\ttransString = getEntropy(tpath, parseChars)\n",
    "\n",
    "\t\toriginalCol.append(origString)\n",
    "\t\ttranslateCol.append(transString)\n",
    "\t\t\n",
    "\t\t# try:\n",
    "\t\t# \tdifferenceCol.append(float(getEntropy(opath))- float(getEntropy(tpath)))\t\n",
    "\t\t# except Exception as e:\n",
    "\t\t# \tpass # Format error \n",
    "\t\t\n",
    "\n",
    "\tassert(len(originalCol) ==  len(translateCol) == df.shape[0])\n",
    "\tdf[\"Original Entropy\"] = originalCol\n",
    "\tdf[\"Translated Entropy\"] = translateCol\n",
    "\t# df[\"Change in E\"] = differenceCol\n",
    "\t# header = columns = ['scpId','href','englishRating','englishDate','englishAuthor','chineseRating','chineseDate','chineseAuthor','chineseAuthorKudos','Original E','Translated E']\n",
    "\t# df.insert(11, \"Change in Entropy\", differenceCol, allow_duplicates=True)\n",
    "\t\n",
    "\toutfile = \"data/kld/\" + lang + \"-entropy.csv\"\n",
    "\tdf.to_csv(outfile, index = False)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\tfiles = [\"data/chinese.csv\", \"data/french.csv\", \"data/german.csv\",\n",
    "\t\"data/italian.csv\", \"data/japanese.csv\", \"data/korean.csv\",\n",
    "\t\"data/portuguese.csv\",\"data/russian.csv\",\"data/spanish.csv\",\n",
    "\t\"data/thai.csv\"]\n",
    "\n",
    "\tfor file in files:\n",
    "\t\tif file == \"data/chinese.csv\" or file == \"data/korean.csv\" or  file == \"data/thai.csv\" or file == \"data/japanese.csv\":\n",
    "\t\t\tparse(file, parseChars = True)\n",
    "\t\telse: \n",
    "\t\t\tparse(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
